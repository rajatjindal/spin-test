interface llm {
	/// A Large Language Model.
	variant model {
		llama2-v70b-chat,
		llama2-v13b-chat,
		llama2-v7b-chat,
		other(string)
	}

	/// Inference request parameters
	record inferencing-params {
		/// The maximum tokens that should be inferred.
		///
		/// Note: the backing implementation may return less tokens.
		max-tokens: u32,
		repeat-penalty: float32,
		repeat-penalty-last-n-token-count: u32,
		temperature: float32,
		top-k: u32,
		top-p: float32
	}

	// The set of errors which may be raised by functions in this interface
	variant error {
		model-not-supported,
		runtime-error(string),
		invalid-input(string)
	}

	// An inferencing result
	// TODO: this should be a stream
	type inferencing-result = string

	// One-shot inferencing operation
	infer: func(model: model, prompt: string, params: option<inferencing-params>) -> result<inferencing-result, error>

	variant embedding-model {
		all-mini-lm-l6-v2
	}

	generate-embeddings: func(model: embedding-model, data: list<string>) -> result<list<list<float32>>, error>
}

